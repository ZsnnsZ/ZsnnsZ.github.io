<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="一句话描述算法损失函数定义输入是训练集，监督学习以带标签的特征向量作为输入，输出是类别(分类)，预测值(回归)使用场景优缺点调参
算法要从以下几个方面来掌握:

产生背景，适用场合（数据规模，特征维度，是否有 Online 算法，离散/连续特征处理等角度）；
原理推导
求解方法（随机梯度下降、拟牛顿法等优化算法）；
优缺点，相关改进；

高频话题是 SVM、LR、决策树（决策森林）和聚类算法是否有">
<meta property="og:type" content="article">
<meta property="og:title" content="[ML]ML算法小结">
<meta property="og:url" content="http://yoursite.com/2017/04/15/ML-ML算法小结/index.html">
<meta property="og:site_name" content="ZSN码疯窝">
<meta property="og:description" content="一句话描述算法损失函数定义输入是训练集，监督学习以带标签的特征向量作为输入，输出是类别(分类)，预测值(回归)使用场景优缺点调参
算法要从以下几个方面来掌握:

产生背景，适用场合（数据规模，特征维度，是否有 Online 算法，离散/连续特征处理等角度）；
原理推导
求解方法（随机梯度下降、拟牛顿法等优化算法）；
优缺点，相关改进；

高频话题是 SVM、LR、决策树（决策森林）和聚类算法是否有">
<meta property="og:updated_time" content="2017-04-15T05:27:50.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="[ML]ML算法小结">
<meta name="twitter:description" content="一句话描述算法损失函数定义输入是训练集，监督学习以带标签的特征向量作为输入，输出是类别(分类)，预测值(回归)使用场景优缺点调参
算法要从以下几个方面来掌握:

产生背景，适用场合（数据规模，特征维度，是否有 Online 算法，离散/连续特征处理等角度）；
原理推导
求解方法（随机梯度下降、拟牛顿法等优化算法）；
优缺点，相关改进；

高频话题是 SVM、LR、决策树（决策森林）和聚类算法是否有">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/04/15/ML-ML算法小结/"/>





  <title> [ML]ML算法小结 | ZSN码疯窝 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  














  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">ZSN码疯窝</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
    
      <p class="site-subtitle"></p>
    
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404.html" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            公益404
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/15/ML-ML算法小结/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="James ZsnnsZ">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="ZSN码疯窝">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="ZSN码疯窝" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                [ML]ML算法小结
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-15T13:09:00+08:00">
                2017-04-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>一句话描述算法<br>损失函数定义<br>输入是训练集，监督学习以带标签的特征向量作为输入，输出是类别(分类)，预测值(回归)<br>使用场景<br>优缺点<br>调参</p>
<p>算法要从以下几个方面来掌握:</p>
<ul>
<li>产生背景，适用场合（数据规模，特征维度，是否有 Online 算法，离散/连续特征处理等角度）；</li>
<li>原理推导</li>
<li>求解方法（随机梯度下降、拟牛顿法等优化算法）；</li>
<li>优缺点，相关改进；</li>
</ul>
<p>高频话题是 SVM、LR、决策树（决策森林）和聚类算法<br>是否有online算法：<br>随机梯度下降(SGD)，生成模型(HMM，朴素贝叶斯)</p>
<a id="more"></a>
<h3 id="监督学习和无监督学习："><a href="#监督学习和无监督学习：" class="headerlink" title="监督学习和无监督学习："></a>监督学习和无监督学习：</h3><p>监督学习，教计算机如何完成任务，给定一个带标签的数据集，这个数据集由正确答案组成。</p>
<p>回归和分类是监督学习的核心问题</p>
<p>预测连续值叫回归<br>回归的算法有：线性回归，多项式回归，岭回归(L2)、lasso回归(L1)、elasticnet回归（1范数、2范数）</p>
<p>预测离散值叫做分类<br>分类的算法有；KNN，感知机，朴素贝叶斯、决策树、LR、SVM、boosting、bagging、贝叶斯网络、神经网络</p>
<p>无监督学习，计算机自己学习，没有正确答案，如Kmeans，非负矩阵因式分解NMF，自组织映射聚类SOM</p>
<h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><p>假设输入X和输出Y符合线性关系，X的每个分量都对应一个未知参数，构造一个线性回归方程来拟合数据的方法叫做线性回归<br>￼<br>优点：简单<br>缺点：不能拟合非线性数据<br>场景：数据线性分布，用于回归</p>
<h3 id="逻辑回归、softmax"><a href="#逻辑回归、softmax" class="headerlink" title="逻辑回归、softmax"></a>逻辑回归、softmax</h3><p>逻辑回归是一种分类模型，在线性回归的基础上，使用sigmoid函数将假设函数的输出映射到[0,1]区间上作为概率估计，从而进行分类<br>￼<br>softmax是逻辑回归应用于多分类问题(one-hot)的扩展，可以使用k个LR多分类<br>多分类问题的第二种解法：训练k个二分类器，预测时将他们都运行一遍，取概率最大值为类别<br>选择k个二分类器和softmax回归的选择关键是目标分类是否互斥，互斥选k个二分类，有共通之处用softmax，softmax的特点是每个分类都有一定概率</p>
<p>优点：易于理解和实现；计算代价不高，速度很快，存储资源低；</p>
<p>缺点：容易欠拟合，分类精度可能不高。只能处理二分类问题(softmax多分类)，特征要求线性可分，非线性特征需要转换(不如用SVM)。当特征空间很大时，逻辑回归的性能不是很好；<br>场景：二分类(设定阈值)</p>
<h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><p>svm是定义在特征空间上的几何间隔最大化的二类分类模型，使用核函数将输入空间的点映射到特征空间，在特征空间利用线性分类，从而将低维空间的超曲面分隔问题变为高维空间的超平面分隔问题<br>损失函数定义有两种方式，一是有约束的优化问题(几何间隔取1，因为它不影响不等式的约束条件和目标函数优化结果)，二是定义在误分类代价的函数(hinge loss，在LR基础上变形)<br>￼￼<br>分析：引入对偶算法是为了结合拉格朗日条件和kkt条件，自然使用核函数，求解更简单</p>
<p>优点：可以解决大量特征的样本，能够处理非线性特征的相关性，泛化能力好，思想简单</p>
<p>缺点：缺失值敏感，样本集太大时效率不算高，核函数选择难</p>
<p>优化：使用SMO方法快速求解SVM，不断的将原凸二次规划问题分解为只有两个变量的二次规划子问题，解析求解子问题，直到所有变量都满足kkt条件</p>
<p>场景：可用于线性／非线性分类，也可用于回归。文本分类，图像识别。</p>
<h3 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h3><p>朴素贝叶斯是基于贝叶斯定理与特征条件独立性假设的一种分类方法，通过计算先验概率，基于条件独立性假设计算条件概率，使用贝叶斯估计计算其后验概率，取最大后验概率的分类作为输出</p>
<p>分析：使用贝叶斯估计，防止某个分类特征为0，全概率公式，当lambda为1时称为拉普拉斯平滑</p>
<p>优点：朴素贝叶斯属于高偏差／低方差模型，对于小规模数据表现较好，能够处理多分类任务，适合增量式训练。</p>
<p>缺点：因为条件独立性假设，需要特征之间无关联，对输入数据很敏感</p>
<p>场景：文本分类</p>
<h3 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h3><p>人工神经网络算法模拟生物神经网络，是一类模式匹配算法。通常用于解决分类和回归问题。<br>神经网络的计算负荷比线性回归或者逻辑回归要小<br>神经网络的输出层和上一个隐藏层的关系就是LR(二分类)或者softmax(多分类)<br>神经网络隐藏层使用的激活函数有sigmoid，relu等</p>
<p>使用神经网络时的步骤:<br>网络结构:第一件要做的事是选择网络结构，即决定选择多少层以及决定每层分别有多少个单元。<br>第一层的单元数即我们训练集的特征数量。 最后一层的单元数是我们训练集的结果的类的数量。如果隐藏层数大于1，确保每个隐藏层的单元个数相同，通常情况下隐藏层单元的个数越多越好。<br>我们真正要决定的是隐藏层的层数和每个中间层的单元数。</p>
<p>训练神经网络:</p>
<pre><code>1.  参数的随机初始化 
2.  利用正向传播方法计算所有的 hθ(x) 
3.  编写计算代价函数 J 的代码 
4.  利用反向传播方法计算所有偏导数 
5.  利用数值检验方法检验这些偏导数 
6.  使用优化算法来最小化代价函数 
</code></pre><p>优点：分类的准确度高；并行分布处理能力强,分布存储及学习能力强，对噪声神经有较强的鲁棒性和容错能力，能充分逼近复杂的非线性关系；具备联想记忆的功能。</p>
<p>缺点：神经网络需要大量的参数，如网络拓扑结构、权值和阈值的初始值；不能观察之间的学习过程，输出结果难以解释，会影响到结果的可信度和可接受程度；学习时间过长,甚至可能达不到学习的目的</p>
<p>场景：分类与回归</p>
<h3 id="k-nn"><a href="#k-nn" class="headerlink" title="k-nn"></a>k-nn</h3><p>从训练样本中找出k个与待分类项最相近的样本，然后这些样本进行投票决定其类别</p>
<p>三要素：k值的选择，距离的度量，分类决策规则（投票）</p>
<p>分析：k越小模型越复杂，通过不同k误差率进行计算选进行k的选择</p>
<p>优点：简单、对outlier不敏感。 可用于非线性分类。用于回归时可以对k个点进行加权平均</p>
<p>缺点：计算量大，样本不平衡敏感导致预测偏差较大。</p>
<p>优化：使用k-d树(实例数量远大于特征数)，将计算距离的空间限制在局部空间</p>
<p>场景：文本分类、模式识别、聚类分析，多分类领域。</p>
<h3 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h3><p>是一种迭代的决策树算法，该算法由多棵决策树(只能是回归树，CART)组成。</p>
<p>回归：假设我们前一轮迭代得到的强学习器是ft−1(x)，损失函数是L(y,ft−1(x)), 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器ht(x)，让本轮的损失最小<br>L(y,ft(x))=L(y,ft−1(x)+ht(x))最小。也就是说，本轮迭代找到决策树，要让样本的总损失尽量变得更小。每一轮的损失函数的负梯度可以近似代表预测与真实情况的差距，下一轮就训练出一个弱学习器来拟合这个损失，那么下一轮得到的强学习器就是上一轮的强学习器加上训练出的CART弱学习器。迭代M次最终得到M个弱学习器，这些弱学习器叠加起来就得到了用于预测的强学习器。总结起来，每一轮迭代就是不断的训练弱学习器拟合残差，更新强学习器。</p>
<p>分类：分类算法和回归算法的差别在于如何拟合残差，因为输出是离散值，所以无法用输出的类别来拟合残差，所以要用类别的预测概率值和真实概率值的差来拟合损失。</p>
<p>总的思想：用损失函数的负梯度来拟合本轮损失的近似值<br>对于分类算法，损失函数一般是指数(退化为Adaboost)或对数(二分类，多分类)<br>对于回归算法，损失函数一般有均方差，绝对值损失，huber，分位数</p>
<p>优点：精度高，能处理各种类型数据，对异常点鲁棒性强(Huber对于远离中心的异常点用绝对损失，中心附近用均方差，分位数损失函数)</p>
<p>缺点：对outlier敏感，并行麻烦(子采样，部分并行)，多分类时复杂度大<br>应用场景：线性／非线性回归(均方差)，二分类(log损失函数)，多分类，搜索排序</p>
<p>调参：<br>迭代次数e_estimators(定)—&gt;决策树最大深度max_depth(定),内部结点再划分所需最小样本数min_samples_split—&gt;内部结点再划分所需最小样本数min_samples_split(定),叶子结点最少样本数min_samples_leaf(定)—&gt;最大特征数max_features(定)—&gt;子采样比例(提升泛化能力，不放回采样)(定)—&gt;减小步长learning_rate增加迭代次数e_estimators可以增强泛化能力，步长太小会导致欠拟合<br>树的个数 100~10000<br>叶子的深度 3~8<br>学习速率 0.01~1<br>叶子上最大节点树 20<br>训练采样比例 0.5~1<br>训练特征采样比例 sqrt(num)</p>
<p>分析：<br>每次迭代训练弱分类器时采用负梯度拟合，损失函数的负梯度来拟合本轮损失的近似值，拟合出一棵CART，与前一轮的强分类器融合为本轮的强分类器<br>正则化防止过拟合的三种方法：设定步长，子采样，对弱学习器(CART)进行剪枝</p>
<h3 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h3><p>采用bagging的思想，改进之处：使用CART，在训练每一个CART决策树的每一个节点时选择该节点的部分特征中的最优特征进行划分，即特征子采样比例（默认为特征数的平方根，该比例越小，模型越健壮，方差减小，偏差变大）</p>
<p>随机森林是利用训练出多棵树(互相无关联，不需要剪枝，可以是回归树，也可以是分类树，一般选CART既可回归又可分类)对样本进行投票从而进行分类的分类器(回归就取分类后的结果的平均)。每棵树要用到所有样本和部分特征。</p>
<p>分析：部分特征一般符合m = sqrt(M)</p>
<p>优点：训练可以高并行化，可以处理高维特征，训练后能给出重要特征，能发现特征之间的关联，实现简单，对outlier不敏感</p>
<p>缺点：在噪音大的样本集上，容易过拟合，</p>
<p>场景：大量特征</p>
<h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><p>决策树的训练是从训练集中归纳出一组分类规则，常用的有ID3(选择分裂后信息增益最大的特征进行分裂)，C4.5(信息增益比)，CART(回归时采用均方差最小化准则，对分类树采用基尼指数（Gini Index）最小化准则，进行特征选择，生成二叉树)</p>
<p>熵：-sigma(pilogpi)<br>gini：1-sigma(pi*pi)</p>
<p>优点：思想简单，计算复杂度低，对数据不敏感(数值数据或者非数值数据均可)，决策过程易于解释、允许数据缺失(对子分支进行加权)、可以处理不相关特征数据。 </p>
<p>缺点：容易过拟合(采取剪枝)，输出结果种类太多时有效性降低，丢失了特征间的相关性。信息增益的结果偏向于那些具有更多数值的特征</p>
<p>场景：带有分界点的大量分类和数值数据共同组成的数据集，如果对决策过程有要求更好。</p>
<h3 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h3><p>感知机是二类分类的线性分类模型</p>
<p>损失函数：误分类点到超平面的总距离</p>
<p>输入：实例的特征向量</p>
<p>输出：实例的类别（+1，-1）</p>
<p>场景：二分类</p>
<h3 id="EM"><a href="#EM" class="headerlink" title="EM"></a>EM</h3><p>EM是一种迭代算法，用于含有隐变量的概率模型参数的极大似然估计。每次迭代分为两步，E求期望，利用对隐藏变量的现有估计值，计算其最大似然估计值，M求极大，最大化在E步骤中的最大似然值，得到参数估计，又用于下一次迭代。直至收敛。</p>
<h3 id="k-means"><a href="#k-means" class="headerlink" title="k-means"></a>k-means</h3><p>以空间中k个点为中心进行聚类，对最靠近他们的对象归类。通过迭代的方法，逐次更新各聚类中心的值，直至得到最好的聚类结果。</p>
<p>优点：算法简单，对大数据集处理效率高，当簇是密集的、球状或团状的，且簇与簇之间区别明显时，聚类效果较好。</p>
<p>缺点：对数据类型要求较高，适合数值型数据；可能收敛到局部最小值，在大规模数据上收敛较慢，K值比较难以选取；对初值的簇心值敏感，对于不同的初始值，可能会导致不同的聚类结果；不适合于发现非凸面形状的簇，或者大小差别很大的簇。对于”噪声”和孤立点数据敏感，少量的该类数据能够对平均值产生极大影响。</p>
<p>优化：K-means++，选择相互之间距离足够大的k个点作为初始点</p>
<p>场景：聚类，进行分组</p>
<h3 id="adaboost"><a href="#adaboost" class="headerlink" title="adaboost"></a>adaboost</h3><p>Adaboost是一种迭代算法，其核心思想是针对同一个训练集训练不同的分类器(使用最广泛的Adaboost弱学习器是决策树和神经网络。对于决策树，Adaboost分类用了CART分类树，而Adaboost回归用了CART回归树)，然后把这些弱分类器集合起来，构成一个更强的最终分类器（强分类器）。提高前一轮弱分类器错误分类的样本的权值，降低被正确分类的样本的权值，对于分类加权多数表决，加大分类误差率小的弱分类器的权重。对于回归加权平均。</p>
<p>Adaboost的主要优点有：<br>1）Adaboost作为分类器时，分类精度很高<br>2）在Adaboost的框架下，可以使用各种回归分类模型来构建弱学习器，非常灵活。<br>3）作为简单的二元分类器时，构造简单，结果可理解。<br>4）不容易发生过拟合</p>
<p>Adaboost的主要缺点有：<br>1）对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。</p>
<p>场景：模式识别，计算机视觉，二分类和多分类</p>
<h3 id="bagging"><a href="#bagging" class="headerlink" title="bagging"></a>bagging</h3><p>有放回随机采样，对m大小的测试集随机抽取m次，有部分会重复，有部分为袋外数据不会被抽取(约占1/3)，造成m个大小但实际用到训练集中的样本只有一部分，来训练一个弱学习器，弱学习器一般为决策树或者神经网络，因为训练简单</p>
<p>优点：泛化能力相对强<br>缺点：拟合效果相对差</p>
<h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>梯度下降的思想是：开始随机选择一个参数的组合(θ0,θ1,…,θn)，计算代价函数，然后寻找下一个能让代价函数下降最多的参数组合(梯度下降)，持续直到找到一个局部最小值。选择不同的初始参数组合可能会找到不同的局部最小值。<br>同时更新，每次都同时让所有参数减去学习速率乘以代价函数的导数<br>学习速率大小的问题：太小，下降速率慢，太大，可能越过最低点，甚至导致无法收敛。<br>可以尝试的学习速率：0.01，0.03，0.1，0.3，1，3，10</p>
<p>最小二乘法，θ = (XT X)<strong>(-1)XT Y对于某些线性回归问题，正规方程是更好的解决方案，数据量较大时，梯度下降比正规方程适用。因为矩阵逆的计算时间复杂度为O(N</strong>3),当特征数量小于10000时可以接受正规方程</p>
<p>特征缩放：在多维特征问题时，要保证这些特征都具有相近的尺度，可以让梯度下降更快收敛。</p>
<h3 id="随机梯度下降（SGD）："><a href="#随机梯度下降（SGD）：" class="headerlink" title="随机梯度下降（SGD）："></a>随机梯度下降（SGD）：</h3><p>在随机梯度下降法中，我们定义代价函数为一个单一训练实例的代价:<br>￼<br>随机梯度下降算法为:首先对训练集随机“洗牌”，然后随机抽样一个样本进行参数的更新</p>
<p>随机梯度下降算法在每一次计算之后便更新参数 θ，而不需要首先将所有的训练集求和<br>算法存在的问题是，不是每一步都是朝着”正确”的方向迈出的。因此算法虽然会逐渐走向全 局最小值的位置，但是可能无法站到那个最小值的那一点，而是在最小值点附近徘徊。<br>在神经网络训练过程中，很少使用随机梯度下降，因为小批量梯度下降可以利用矩阵和向量计算进行加速。通常我们讲随机梯度下降（SGD）时，也可以指代小批量梯度下降（mini batch）。</p>
<h3 id="其他优化"><a href="#其他优化" class="headerlink" title="其他优化"></a>其他优化</h3><p>提升梯度下降法收敛速度的方法还包括将其由一阶提升为二阶，也就是牛顿法或者拟牛顿法，比如常用的 L-BFGS。<br>然而，牛顿法和L-BFGS不适用于解决大规模训练数据集和大规模问题。<br>L-BFGS法不需要计算完全的Hessian海森矩阵，虽然没有了内存的担忧，但这种方法通常类似于批量梯度下降法，需要在计算整个训练集（通常为几百万个样本）的梯度后才能更新一次参数，严重影响了收敛速度。因而在深度神经网络领域很少使用L-BFGS来优化目标函数。</p>
<p>动量mom在迭代开始时将其设为0.5，在20次迭代（epoch）后，其值更新为0.99。<br>学习率的更新采用逐步降低（Step decay）法，即每次迭代（epoch）后将学习率乘以0.5。</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/04/05/ML-过拟合/" rel="next" title="[ML]过拟合">
                <i class="fa fa-chevron-left"></i> [ML]过拟合
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>

          
          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.png"
               alt="James ZsnnsZ" />
          <p class="site-author-name" itemprop="name">James ZsnnsZ</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">31</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/ZsnnsZ" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#监督学习和无监督学习："><span class="nav-number">1.</span> <span class="nav-text">监督学习和无监督学习：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#线性回归"><span class="nav-number">2.</span> <span class="nav-text">线性回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#逻辑回归、softmax"><span class="nav-number">3.</span> <span class="nav-text">逻辑回归、softmax</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SVM"><span class="nav-number">4.</span> <span class="nav-text">SVM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#朴素贝叶斯"><span class="nav-number">5.</span> <span class="nav-text">朴素贝叶斯</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络"><span class="nav-number">6.</span> <span class="nav-text">神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#k-nn"><span class="nav-number">7.</span> <span class="nav-text">k-nn</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GBDT"><span class="nav-number">8.</span> <span class="nav-text">GBDT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#随机森林"><span class="nav-number">9.</span> <span class="nav-text">随机森林</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#决策树"><span class="nav-number">10.</span> <span class="nav-text">决策树</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#感知机"><span class="nav-number">11.</span> <span class="nav-text">感知机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#EM"><span class="nav-number">12.</span> <span class="nav-text">EM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#k-means"><span class="nav-number">13.</span> <span class="nav-text">k-means</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#adaboost"><span class="nav-number">14.</span> <span class="nav-text">adaboost</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bagging"><span class="nav-number">15.</span> <span class="nav-text">bagging</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度下降"><span class="nav-number">16.</span> <span class="nav-text">梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#随机梯度下降（SGD）："><span class="nav-number">17.</span> <span class="nav-text">随机梯度下降（SGD）：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#其他优化"><span class="nav-number">18.</span> <span class="nav-text">其他优化</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">James ZsnnsZ</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  




	





  





  





  



  
  

  

  

  

  


</body>
</html>
